# xAILab
# Chair of Explainable Machine Learning
# Otto-Friedrich University of Bamberg
#
# Parameter configurations for training and evaluation.

# Fixed Parameters
data_path: '<data_path>'  # Where the dataset is stored, if stored locally.
output_path: '<output_path>'  # Where the trained model shall be stored.
epochs: 100  # How many epochs to train for.
early_stopping_epochs: 10 # Number of epochs after which training should be halted if no further improvement in training
use_lr_scheduler: True
learning_rate: 0.0001  # Learning rate
batch_size: 64  # Batch size for the training.
# batch_size_eval: 256  # Batch size for the evaluation.
optimizer: 'AdamW' # Optimizer to be used
device: 'cuda:0'  # Which device to run the computations on.
num_workers: 4
weighted_loss: True # using this the loss gets weighted in order to prevent the gradient from exploding for losses
train_backbone: True # If one wants to train just the multiple heads without the shared backbone -> set to False
backbone_pretrained: True
# computed on batches for rarely seen datasets

# Modifiable Parameters
img_size: 224  # Height and width of the input image. (28, 64, 128, 224)
architecture_name: 'vit_base_patch16_224' # 'resnet18' 'resnet50' 'densenet121' 'vgg16' 'vit_base_patch16_224'
seed: 9930641  # Seed for random operations for reproducibility. e.g. (9930641, 115149041, 252139603)